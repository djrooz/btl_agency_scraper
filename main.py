#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ BTL –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞—Ö
"""
import os
import json
import time
from pathlib import Path
from typing import List, Dict, Any

from src.utils import main_logger
from src.scrapers import RRARScraper, MarketingTechScraper
from src.scrapers.fns_api_client import FNSAPIClient
from src.processors import DataCleaner, DuplicateHandler
from config import config

def setup_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    directories = [
        "data",
        "data/raw",
        "data/interim",
        "logs"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
    
    main_logger.info("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã/–ø—Ä–æ–≤–µ—Ä–µ–Ω—ã")

def collect_data_from_sources() -> List[Dict[str, Any]]:
    """
    –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
    """
    all_companies = []
    
    main_logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    # 1. –ü–∞—Ä—Å–∏–Ω–≥ –†–†–ê–† —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
    try:
        main_logger.info("–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –†–†–ê–†...")
        rrar_scraper = RRARScraper()
        rrar_companies = rrar_scraper.scrape_all()
        
        if rrar_companies:
            rrar_scraper.save_raw_data(rrar_companies)
            all_companies.extend(rrar_companies)
            main_logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ –∏–∑ –†–†–ê–†: {len(rrar_companies)} –∫–æ–º–ø–∞–Ω–∏–π")
        
    except Exception as e:
        main_logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –†–†–ê–†: {e}")
    
    # 2. –ü–∞—Ä—Å–∏–Ω–≥ marketing-tech.ru
    try:
        main_logger.info("–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ marketing-tech...")
        marketing_scraper = MarketingTechScraper()
        marketing_companies = marketing_scraper.scrape_all()
        
        if marketing_companies:
            marketing_scraper.save_raw_data(marketing_companies)
            all_companies.extend(marketing_companies)
            main_logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ –∏–∑ marketing-tech: {len(marketing_companies)} –∫–æ–º–ø–∞–Ω–∏–π")
        
    except Exception as e:
        main_logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö marketing-tech: {e}")
    
    # 3. –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –§–ù–° API
    try:
        main_logger.info("–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –§–ù–° API...")
        fns_client = FNSAPIClient()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ò–ù–ù –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        inns = set()
        for company in all_companies:
            inn = company.get('inn')
            if inn and len(inn) in [10, 12]:
                inns.add(inn)
        
        if inns:
            main_logger.info(f"–û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(inns)} –ò–ù–ù")
            fns_companies = fns_client.batch_get_companies(list(inns))
            
            if fns_companies:
                fns_client.save_raw_data(fns_companies)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –§–ù–° —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∑–∞–ø–∏—Å—è–º–∏
                all_companies = merge_fns_data(all_companies, fns_companies)
                main_logger.info(f"–û–±–æ–≥–∞—â–µ–Ω–æ –¥–∞–Ω–Ω—ã–º–∏ –§–ù–°: {len(fns_companies)} –∫–æ–º–ø–∞–Ω–∏–π")
        
    except Exception as e:
        main_logger.error(f"–û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –§–ù–°: {e}")
    
    main_logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(all_companies)} –∑–∞–ø–∏—Å–µ–π")
    return all_companies

def merge_fns_data(companies: List[Dict[str, Any]], fns_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π —Å –¥–∞–Ω–Ω—ã–º–∏ –§–ù–°
    
    Args:
        companies: –°–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π
        fns_data: –î–∞–Ω–Ω—ã–µ –§–ù–°
        
    Returns:
        –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π
    """
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ –ò–ù–ù –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    fns_index = {item['inn']: item for item in fns_data if item.get('inn')}
    
    for company in companies:
        inn = company.get('inn')
        if inn and inn in fns_index:
            fns_record = fns_index[inn]
            
            # –û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ, –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            for key, value in fns_record.items():
                if key not in company or not company[key]:
                    company[key] = value
    
    return companies

def process_data(companies: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        companies: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–π
        
    Returns:
        –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–π
    """
    main_logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö")
    
    # 1. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    cleaner = DataCleaner()
    cleaned_companies = cleaner.clean_companies_data(companies)
    
    if cleaned_companies:
        cleaner.save_cleaned_data(cleaned_companies)
    
    # 2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    relevant_companies = cleaner.filter_by_relevance(cleaned_companies)
    
    # 3. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    dedup_handler = DuplicateHandler()
    unique_companies = dedup_handler.remove_duplicates(relevant_companies)
    
    if unique_companies:
        dedup_handler.save_deduplicated_data(unique_companies)
    
    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—ã—Ä—É—á–∫–µ
    final_companies = filter_by_revenue(unique_companies)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    stats = dedup_handler.get_duplicate_statistics(len(relevant_companies), len(unique_companies))
    main_logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {stats}")
    
    main_logger.info(f"–§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(final_companies)}")
    
    return final_companies

def filter_by_revenue(companies: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–ø–∞–Ω–∏–π –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –≤—ã—Ä—É—á–∫–µ
    
    Args:
        companies: –°–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π
        
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π
    """
    min_revenue = config.min_revenue
    filtered = []
    
    for company in companies:
        revenue = company.get('revenue', 0)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ —Å –Ω—É–ª–µ–≤–æ–π –≤—ã—Ä—É—á–∫–æ–π (–¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º–∏)
        # –∏–ª–∏ —Å –≤—ã—Ä—É—á–∫–æ–π –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
        if revenue == 0 or revenue >= min_revenue:
            filtered.append(company)
    
    main_logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤—ã—Ä—É—á–∫–µ ‚â•{min_revenue:,}: {len(filtered)} –∫–æ–º–ø–∞–Ω–∏–π")
    
    return filtered

def generate_final_csv(companies: List[Dict[str, Any]]) -> None:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ CSV —Ñ–∞–π–ª–∞
    
    Args:
        companies: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–π
    """
    try:
        import pandas as pd
        
        if not companies:
            main_logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è CSV")
            return
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(companies)
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = [
            'inn', 'name', 'revenue_year', 'revenue', 'segment_tag', 'source'
        ]
        
        optional_columns = [
            'okved_main', 'employees', 'site', 'description', 'region', 'contacts', 'rating_ref'
        ]
        
        all_columns = required_columns + optional_columns
        
        for col in all_columns:
            if col not in df.columns:
                df[col] = ''
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—ã—Ä—É—á–∫–µ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
        df = df.sort_values('revenue', ascending=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π CSV
        output_file = config.output.get('csv_file', 'data/companies.csv')
        df[all_columns].to_csv(output_file, index=False, encoding='utf-8')
        
        main_logger.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–π CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        main_logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–∞–Ω–∏–π –≤ —Ñ–∞–π–ª–µ: {len(df)}")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print_statistics(df)
        
    except Exception as e:
        main_logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ CSV: {e}")

def print_statistics(df) -> None:
    """
    –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏–π
    """
    print("\n" + "="*60)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–û–ë–†–ê–ù–ù–´–• –î–ê–ù–ù–´–•")
    print("="*60)
    
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(df)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
    if 'segment_tag' in df.columns:
        segment_stats = df['segment_tag'].value_counts()
        print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º:")
        for segment, count in segment_stats.items():
            print(f"  {segment}: {count}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    if 'source' in df.columns:
        source_stats = df['source'].value_counts()
        print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        for source, count in source_stats.items():
            print(f"  {source}: {count}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—ã—Ä—É—á–∫–µ
    if 'revenue' in df.columns:
        revenue_stats = df[df['revenue'] > 0]['revenue']
        if len(revenue_stats) > 0:
            print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—ã—Ä—É—á–∫–µ ({len(revenue_stats)} –∫–æ–º–ø–∞–Ω–∏–π —Å –¥–∞–Ω–Ω—ã–º–∏):")
            print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {revenue_stats.min():,.0f} —Ä—É–±.")
            print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {revenue_stats.max():,.0f} —Ä—É–±.")
            print(f"  –°—Ä–µ–¥–Ω—è—è: {revenue_stats.mean():,.0f} —Ä—É–±.")
            print(f"  –ú–µ–¥–∏–∞–Ω–Ω–∞—è: {revenue_stats.median():,.0f} —Ä—É–±.")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
    if 'region' in df.columns:
        region_stats = df[df['region'] != '']['region'].value_counts().head(5)
        if len(region_stats) > 0:
            print("\n–¢–æ–ø-5 —Ä–µ–≥–∏–æ–Ω–æ–≤:")
            for region, count in region_stats.items():
                print(f"  {region}: {count}")
    
    # –ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n–ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö:")
    for col in ['inn', 'revenue', 'site', 'contacts', 'okved_main']:
        if col in df.columns:
            filled = len(df[df[col] != ''])
            percentage = (filled / len(df)) * 100
            print(f"  {col}: {filled}/{len(df)} ({percentage:.1f}%)")
    
    print("="*60 + "\n")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    start_time = time.time()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ BTL –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞—Ö")
    print("="*60)
    
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        setup_directories()
        
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        companies = collect_data_from_sources()
        
        if not companies:
            main_logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞")
            return
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        processed_companies = process_data(companies)
        
        if not processed_companies:
            main_logger.error("–ù–µ –ø–æ–ª—É—á–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            return
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ CSV
        generate_final_csv(processed_companies)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"\n‚úÖ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"üìä –°–æ–±—Ä–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(processed_companies)}")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {config.output.get('csv_file', 'data/companies.csv')}")
        
    except KeyboardInterrupt:
        main_logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print("\n‚ùå –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        
    except Exception as e:
        main_logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise

if __name__ == "__main__":
    main()
